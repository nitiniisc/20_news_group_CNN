{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from data_utils import *\n",
    "#from cnn import TextCNN\n",
    "embedding_size = 128\n",
    "filter_sizes =[3,4]\n",
    "num_filters = 32\n",
    "num_classes =20\n",
    "batch_size = 64\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(x, ksize):\n",
    "    return tf.nn.max_pool(x, ksize,strides=[1, 1, 1, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training example for few 11314\n",
      "length of training labels for few 11314\n",
      "max length of doc 21273\n",
      "number of classes 20\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = load_data_and_labels()\n",
    "#x_train=x_trai[0:1000]\n",
    "#y_train=y_trai[0:1000]\n",
    "        \n",
    "print(\"length of training example for few\",len(x_train))\n",
    "print(\"length of training labels for few\",len(y_train))\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_train])\n",
    "print(\"max length of doc\",max_document_length)\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(x_train)))\n",
    "x_test = np.array(list(vocab_processor.fit_transform(x_test)))\n",
    "sequence_length=x_train.shape[1]\n",
    "num_classes=y_train.shape[1]\n",
    "vocab_size=len(vocab_processor.vocabulary_)\n",
    "print(\"number of classes\",num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = weight_variable([vocab_size, embedding_size])\n",
    "embedded_chars = tf.nn.embedding_lookup(W,input_x)\n",
    "embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "64\n",
      "(?, 1, 2, 32)\n",
      "(?, 20)\n"
     ]
    }
   ],
   "source": [
    "pooled_outputs = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    # Convolution Layer\n",
    "    filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "    W = weight_variable(filter_shape)\n",
    "    b = bias_variable([num_filters])\n",
    "    conv = conv2d(embedded_chars_expanded, W)\n",
    "    # Apply nonlinearity\n",
    "    h = tf.nn.relu(tf.nn.bias_add(conv, b))\n",
    "    # Maxpooling over the outputs\n",
    "    pooled = max_pool(h, [1, sequence_length - filter_size + 1, 1, 1])\n",
    "    pooled_outputs.append(pooled)\n",
    "print(len(pooled_outputs))    \n",
    "num_filters_total = 32 * len(filter_sizes)\n",
    "print(num_filters_total)\n",
    "h_pool = tf.concat(pooled_outputs,2)\n",
    "print(h_pool.shape)\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "W = weight_variable([num_filters_total, num_classes])\n",
    "b = bias_variable([num_classes])\n",
    "scores = tf.nn.softmax(tf.matmul(h_pool_flat, W) + b)\n",
    "print(scores.shape)\n",
    "pred = tf.nn.softmax(scores)\n",
    "predictions = tf.argmax(scores, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                    logits=scores, labels=input_y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "correct_predictions = tf.equal(predictions, tf.argmax(input_y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    print(data.shape)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 21273)\n",
      "(11314, 20)\n",
      "11314\n",
      "(11314, 2)\n",
      "Loss: 2.9932801723480225\n",
      "Loss: 2.995255470275879\n",
      "Loss: 2.9988856315612793\n",
      "Loss: 2.9962081909179688\n",
      "Loss: 2.995871067047119\n",
      "Loss: 2.9895544052124023\n",
      "Loss: 2.9961633682250977\n",
      "Loss: 2.9995310306549072\n",
      "Loss: 2.9942736625671387\n",
      "Loss: 2.9964005947113037\n",
      "Loss: 2.990593433380127\n",
      "Loss: 2.999326705932617\n",
      "Loss: 2.9931840896606445\n",
      "Loss: 2.9911985397338867\n",
      "Loss: 2.995427131652832\n",
      "Loss: 2.9969229698181152\n",
      "Loss: 2.993962287902832\n",
      "Loss: 2.989734172821045\n",
      "Loss: 2.9917750358581543\n",
      "Loss: 2.9932680130004883\n",
      "Loss: 3.001214027404785\n",
      "Loss: 2.9912376403808594\n",
      "Loss: 2.9941539764404297\n",
      "Loss: 2.9958415031433105\n",
      "Loss: 2.9920949935913086\n",
      "Loss: 2.989774227142334\n",
      "Loss: 2.9956130981445312\n",
      "Loss: 2.995276689529419\n",
      "Loss: 2.9890284538269043\n",
      "Loss: 2.9984796047210693\n",
      "Loss: 2.9909555912017822\n",
      "Loss: 2.990151882171631\n",
      "Loss: 2.9920740127563477\n",
      "Loss: 2.99271821975708\n",
      "Loss: 2.99293851852417\n",
      "Loss: 2.9907948970794678\n",
      "Loss: 2.994096279144287\n",
      "Loss: 2.9962825775146484\n",
      "Loss: 2.9914541244506836\n",
      "Loss: 2.9945425987243652\n",
      "Loss: 2.986280918121338\n",
      "Loss: 2.9920315742492676\n",
      "Loss: 2.9953575134277344\n",
      "Loss: 2.9875285625457764\n",
      "Loss: 2.9872517585754395\n",
      "Loss: 2.9871761798858643\n",
      "Loss: 2.98954701423645\n",
      "Loss: 2.9888970851898193\n",
      "Loss: 2.989302396774292\n",
      "Loss: 2.976937770843506\n",
      "Loss: 2.9902737140655518\n",
      "Loss: 2.9822542667388916\n",
      "Loss: 2.9905037879943848\n",
      "Loss: 2.9927921295166016\n",
      "Loss: 2.9884419441223145\n",
      "Loss: 2.9761526584625244\n",
      "Loss: 2.9952993392944336\n",
      "Loss: 2.989362955093384\n",
      "Loss: 2.9983646869659424\n",
      "Loss: 2.986412763595581\n",
      "Loss: 2.9916253089904785\n",
      "Loss: 2.9772281646728516\n",
      "Loss: 2.9740049839019775\n",
      "Loss: 2.99114990234375\n",
      "Loss: 2.973642349243164\n",
      "Loss: 2.9929275512695312\n",
      "Loss: 2.9668397903442383\n",
      "Loss: 2.9830799102783203\n",
      "Loss: 2.980762481689453\n",
      "Loss: 2.996019124984741\n",
      "Loss: 2.9955832958221436\n",
      "Loss: 2.991891860961914\n",
      "Loss: 2.9928741455078125\n",
      "Loss: 2.983323574066162\n",
      "Loss: 2.975986957550049\n",
      "Loss: 2.9954476356506348\n",
      "Loss: 2.9847142696380615\n",
      "Loss: 2.9728317260742188\n",
      "Loss: 3.0020809173583984\n",
      "Loss: 2.9714102745056152\n",
      "Loss: 2.9894962310791016\n",
      "Loss: 2.980700969696045\n",
      "Loss: 2.9694437980651855\n",
      "Loss: 2.9957330226898193\n",
      "Loss: 2.9964094161987305\n",
      "Loss: 2.9909660816192627\n",
      "Loss: 2.9966890811920166\n",
      "Loss: 2.9720044136047363\n",
      "Loss: 2.9788365364074707\n",
      "Loss: 2.9673781394958496\n",
      "Loss: 2.9797730445861816\n",
      "Loss: 2.982187032699585\n",
      "Loss: 2.975944757461548\n",
      "Loss: 2.987182140350342\n",
      "Loss: 2.9653780460357666\n",
      "Loss: 2.9880266189575195\n",
      "Loss: 2.9795289039611816\n",
      "Loss: 2.9637269973754883\n",
      "Loss: 2.9734818935394287\n",
      "Loss: 2.986994504928589\n",
      "Loss: 2.944862127304077\n",
      "Loss: 2.957671642303467\n",
      "Loss: 2.9843392372131348\n",
      "Loss: 2.9800760746002197\n",
      "Loss: 2.9663071632385254\n",
      "Loss: 2.977107048034668\n",
      "Loss: 2.9814019203186035\n",
      "Loss: 2.965235471725464\n",
      "Loss: 2.9432084560394287\n",
      "Loss: 2.977238178253174\n",
      "Loss: 2.9372386932373047\n",
      "Loss: 2.985013484954834\n",
      "Loss: 2.9827585220336914\n",
      "Loss: 2.937997817993164\n",
      "Loss: 2.948746681213379\n",
      "Loss: 2.981083393096924\n",
      "Loss: 2.944234848022461\n",
      "Loss: 2.950493574142456\n",
      "Loss: 2.9656238555908203\n",
      "Loss: 2.952515125274658\n",
      "Loss: 2.9606690406799316\n",
      "Loss: 2.971400022506714\n",
      "Loss: 2.9617745876312256\n",
      "Loss: 2.9626970291137695\n",
      "Loss: 2.9652140140533447\n",
      "Loss: 2.980102062225342\n",
      "Loss: 2.9183719158172607\n",
      "Loss: 2.9839587211608887\n",
      "Loss: 2.9538936614990234\n",
      "Loss: 2.917235851287842\n",
      "Loss: 2.9324655532836914\n",
      "Loss: 2.9020795822143555\n",
      "Loss: 2.958866596221924\n",
      "Loss: 2.956535577774048\n",
      "Loss: 2.9323010444641113\n",
      "Loss: 2.96105694770813\n",
      "Loss: 2.9266796112060547\n",
      "Loss: 2.939767837524414\n",
      "Loss: 2.9190807342529297\n",
      "Loss: 2.9288134574890137\n",
      "Loss: 2.9470887184143066\n",
      "Loss: 2.9229249954223633\n",
      "Loss: 2.941329002380371\n",
      "Loss: 2.9416444301605225\n",
      "Loss: 2.8991341590881348\n",
      "Loss: 2.8918616771698\n",
      "Loss: 2.8755857944488525\n",
      "Loss: 2.9247984886169434\n",
      "Loss: 2.8944010734558105\n",
      "Loss: 2.9243061542510986\n",
      "Loss: 2.9423937797546387\n",
      "Loss: 2.8983325958251953\n",
      "Loss: 2.8893728256225586\n",
      "Loss: 2.9221105575561523\n",
      "Loss: 2.969862461090088\n",
      "Loss: 2.825407028198242\n",
      "Loss: 2.916323661804199\n",
      "Loss: 2.85964298248291\n",
      "Loss: 2.9174790382385254\n",
      "Loss: 2.9121406078338623\n",
      "Loss: 2.912844657897949\n",
      "Loss: 2.896368980407715\n",
      "Loss: 2.813112497329712\n",
      "Loss: 2.9033844470977783\n",
      "Loss: 2.8501055240631104\n",
      "Loss: 2.866642475128174\n",
      "Loss: 2.9032037258148193\n",
      "Loss: 2.8900105953216553\n",
      "Loss: 2.898212432861328\n",
      "Loss: 2.9091858863830566\n",
      "Loss: 2.8944573402404785\n",
      "Loss: 2.906338691711426\n",
      "Loss: 2.8582801818847656\n",
      "Loss: 2.91534686088562\n",
      "Loss: 2.87479305267334\n",
      "Loss: 2.8987388610839844\n",
      "Loss: 2.7946794033050537\n",
      "Loss: 2.8783411979675293\n",
      "Loss: 2.880396604537964\n",
      "Loss: 2.9134790897369385\n",
      "Loss: 2.8970181941986084\n",
      "Loss: 2.873434543609619\n",
      "Loss: 2.7799794673919678\n",
      "Loss: 2.9223084449768066\n",
      "Loss: 2.8402717113494873\n",
      "Loss: 2.87847900390625\n",
      "Loss: 2.8251733779907227\n",
      "Loss: 2.819459915161133\n",
      "Loss: 2.8258211612701416\n",
      "Loss: 2.8259811401367188\n",
      "Loss: 2.8381853103637695\n",
      "Loss: 2.772728204727173\n",
      "Loss: 2.8134703636169434\n",
      "Loss: 2.7756710052490234\n",
      "Loss: 2.8083417415618896\n",
      "Loss: 2.8203577995300293\n",
      "Loss: 2.827543258666992\n",
      "Loss: 2.8532238006591797\n",
      "Loss: 2.875955581665039\n",
      "Loss: 2.8328654766082764\n",
      "Loss: 2.8080227375030518\n",
      "Loss: 2.813284397125244\n",
      "Loss: 2.7228667736053467\n",
      "Loss: 2.7794792652130127\n",
      "Loss: 2.7406649589538574\n",
      "Loss: 2.706778049468994\n",
      "Loss: 2.7885799407958984\n",
      "Loss: 2.80592942237854\n",
      "Loss: 2.8551759719848633\n",
      "Loss: 2.798067569732666\n",
      "Loss: 2.7990307807922363\n",
      "Loss: 2.7403576374053955\n",
      "Loss: 2.798928737640381\n",
      "Loss: 2.7726993560791016\n",
      "Loss: 2.790311813354492\n",
      "Loss: 2.7851243019104004\n",
      "Loss: 2.8256890773773193\n",
      "Loss: 2.706526279449463\n",
      "Loss: 2.77061128616333\n",
      "Loss: 2.7732396125793457\n",
      "Loss: 2.7618396282196045\n",
      "Loss: 2.7520341873168945\n",
      "Loss: 2.7365081310272217\n",
      "Loss: 2.696443557739258\n",
      "Loss: 2.7811713218688965\n",
      "Loss: 2.773564100265503\n",
      "Loss: 2.7530689239501953\n",
      "Loss: 2.789158821105957\n",
      "Loss: 2.739455223083496\n",
      "Loss: 2.6940689086914062\n",
      "Loss: 2.740835666656494\n",
      "Loss: 2.7727296352386475\n",
      "Loss: 2.745997905731201\n",
      "Loss: 2.7111573219299316\n",
      "Loss: 2.7695095539093018\n",
      "Loss: 2.73439359664917\n",
      "Loss: 2.805574655532837\n",
      "Loss: 2.7109899520874023\n",
      "Loss: 2.725421667098999\n",
      "Loss: 2.73576021194458\n",
      "Loss: 2.703220844268799\n",
      "Loss: 2.7073111534118652\n",
      "Loss: 2.723745584487915\n",
      "Loss: 2.752845048904419\n",
      "Loss: 2.64853572845459\n",
      "Loss: 2.6707231998443604\n",
      "Loss: 2.648184061050415\n",
      "Loss: 2.6571927070617676\n",
      "Loss: 2.765124797821045\n",
      "Loss: 2.7696800231933594\n",
      "Loss: 2.6159846782684326\n",
      "Loss: 2.777301549911499\n",
      "Loss: 2.7957167625427246\n",
      "Loss: 2.7607545852661133\n",
      "Loss: 2.671130657196045\n",
      "Loss: 2.666003704071045\n",
      "Loss: 2.7348122596740723\n",
      "Loss: 2.7544384002685547\n",
      "Loss: 2.723271608352661\n",
      "Loss: 2.6791982650756836\n",
      "Loss: 2.722433090209961\n",
      "Loss: 2.6860060691833496\n",
      "Loss: 2.7166223526000977\n",
      "Loss: 2.81083083152771\n",
      "Loss: 2.7593793869018555\n",
      "Loss: 2.693544387817383\n",
      "Loss: 2.624443531036377\n",
      "Loss: 2.6435413360595703\n",
      "Loss: 2.644209384918213\n",
      "Loss: 2.655189037322998\n",
      "Loss: 2.6806700229644775\n",
      "Loss: 2.695957660675049\n",
      "Loss: 2.632317066192627\n",
      "Loss: 2.7394371032714844\n",
      "Loss: 2.6041808128356934\n",
      "Loss: 2.705653429031372\n",
      "Loss: 2.7431907653808594\n",
      "Loss: 2.693211793899536\n",
      "Loss: 2.691612958908081\n",
      "Loss: 2.6046929359436035\n",
      "Loss: 2.7718050479888916\n",
      "Loss: 2.7268576622009277\n",
      "Loss: 2.7677502632141113\n",
      "Loss: 2.6654653549194336\n",
      "Loss: 2.6527047157287598\n",
      "Loss: 2.750314235687256\n",
      "Loss: 2.731637477874756\n",
      "Loss: 2.6846275329589844\n",
      "Loss: 2.730865001678467\n",
      "Loss: 2.7454512119293213\n",
      "Loss: 2.7283401489257812\n",
      "Loss: 2.7129764556884766\n",
      "Loss: 2.668591260910034\n",
      "Loss: 2.6355478763580322\n",
      "Loss: 2.590956211090088\n",
      "Loss: 2.72965407371521\n",
      "Loss: 2.601512908935547\n",
      "Loss: 2.6478676795959473\n",
      "Loss: 2.5750136375427246\n",
      "Loss: 2.6265244483947754\n",
      "Loss: 2.7128400802612305\n",
      "Loss: 2.6852235794067383\n",
      "Loss: 2.7427830696105957\n",
      "Loss: 2.7078003883361816\n",
      "Loss: 2.6232290267944336\n",
      "Loss: 2.635554552078247\n",
      "Loss: 2.632906913757324\n",
      "Loss: 2.6890170574188232\n",
      "Loss: 2.732891798019409\n",
      "Loss: 2.6848769187927246\n",
      "Loss: 2.683074712753296\n",
      "Loss: 2.6496543884277344\n",
      "Loss: 2.522348165512085\n",
      "Loss: 2.637749195098877\n",
      "Loss: 2.5873589515686035\n",
      "Loss: 2.5731425285339355\n",
      "Loss: 2.589778423309326\n",
      "Loss: 2.701603412628174\n",
      "Loss: 2.683603286743164\n",
      "Loss: 2.701533317565918\n",
      "Loss: 2.666398525238037\n",
      "Loss: 2.5821516513824463\n",
      "Loss: 2.5629405975341797\n",
      "Loss: 2.5636098384857178\n",
      "Loss: 2.610276937484741\n",
      "Loss: 2.6601574420928955\n",
      "Loss: 2.6166276931762695\n",
      "Loss: 2.582354784011841\n",
      "Loss: 2.5801212787628174\n",
      "Loss: 2.5978174209594727\n",
      "Loss: 2.4963200092315674\n",
      "Loss: 2.6235780715942383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.6276707649230957\n",
      "Loss: 2.5285849571228027\n",
      "Loss: 2.622931957244873\n",
      "Loss: 2.612987518310547\n",
      "Loss: 2.6378281116485596\n",
      "Loss: 2.671562433242798\n",
      "Loss: 2.659072160720825\n",
      "Loss: 2.6656460762023926\n",
      "Loss: 2.641119956970215\n",
      "Loss: 2.6773128509521484\n",
      "Loss: 2.658395767211914\n",
      "Loss: 2.6429176330566406\n",
      "Loss: 2.515225410461426\n",
      "Loss: 2.6244542598724365\n",
      "Loss: 2.555297613143921\n",
      "Loss: 2.6732020378112793\n",
      "Loss: 2.6799206733703613\n",
      "Loss: 2.5840554237365723\n",
      "Loss: 2.660921812057495\n",
      "Loss: 2.542893409729004\n",
      "Loss: 2.4767441749572754\n",
      "Loss: 2.6388072967529297\n",
      "Loss: 2.5800628662109375\n",
      "Loss: 2.5411911010742188\n",
      "Loss: 2.6165099143981934\n",
      "Loss: 2.540222644805908\n",
      "Loss: 2.575862407684326\n",
      "Loss: 2.5564374923706055\n",
      "Loss: 2.546225070953369\n",
      "Loss: 2.572535514831543\n",
      "Loss: 2.470294237136841\n",
      "Loss: 2.5082149505615234\n",
      "Loss: 2.57454776763916\n",
      "Loss: 2.5707528591156006\n",
      "Loss: 2.645537853240967\n",
      "Loss: 2.501861572265625\n",
      "Loss: 2.575679063796997\n",
      "Loss: 2.5586509704589844\n",
      "Loss: 2.5584452152252197\n",
      "Loss: 2.490914821624756\n",
      "Loss: 2.559995174407959\n",
      "Loss: 2.4852607250213623\n",
      "Loss: 2.5971155166625977\n",
      "Loss: 2.5641589164733887\n",
      "Loss: 2.683971405029297\n",
      "Loss: 2.489321231842041\n",
      "Loss: 2.538733959197998\n",
      "Loss: 2.4830973148345947\n",
      "Loss: 2.436732530593872\n",
      "Loss: 2.5955822467803955\n",
      "Loss: 2.5511832237243652\n",
      "Loss: 2.528675079345703\n",
      "Loss: 2.4444918632507324\n",
      "Loss: 2.447415351867676\n",
      "Loss: 2.679980754852295\n",
      "Loss: 2.60280179977417\n",
      "Loss: 2.6125447750091553\n",
      "Loss: 2.4842755794525146\n",
      "Loss: 2.492518901824951\n",
      "Loss: 2.498685598373413\n",
      "Loss: 2.5398855209350586\n",
      "Loss: 2.5458288192749023\n",
      "Loss: 2.5530481338500977\n",
      "Loss: 2.3723299503326416\n",
      "Loss: 2.519068717956543\n",
      "Loss: 2.5133118629455566\n",
      "Loss: 2.470243453979492\n",
      "Loss: 2.5069456100463867\n",
      "Loss: 2.4797558784484863\n",
      "Loss: 2.4874329566955566\n",
      "Loss: 2.4677670001983643\n",
      "Loss: 2.4653682708740234\n",
      "Loss: 2.4443280696868896\n",
      "Loss: 2.460148811340332\n",
      "Loss: 2.4578685760498047\n",
      "Loss: 2.4704689979553223\n",
      "Loss: 2.518608570098877\n",
      "Loss: 2.460008144378662\n",
      "Loss: 2.501295804977417\n",
      "Loss: 2.5864202976226807\n",
      "Loss: 2.5510525703430176\n",
      "Loss: 2.424398422241211\n",
      "Loss: 2.530942440032959\n",
      "Loss: 2.52567458152771\n",
      "Loss: 2.485077381134033\n",
      "Loss: 2.540304183959961\n",
      "Loss: 2.599442481994629\n",
      "Loss: 2.4645066261291504\n",
      "Loss: 2.4271342754364014\n",
      "Loss: 2.447923183441162\n",
      "Loss: 2.493335485458374\n",
      "Loss: 2.504960775375366\n",
      "Loss: 2.4022183418273926\n",
      "Loss: 2.498342514038086\n",
      "Loss: 2.4473648071289062\n",
      "Loss: 2.4999289512634277\n",
      "Loss: 2.461153984069824\n",
      "Loss: 2.5101895332336426\n",
      "Loss: 2.523736000061035\n",
      "Loss: 2.5222713947296143\n",
      "Loss: 2.4680986404418945\n",
      "Loss: 2.440910816192627\n",
      "Loss: 2.4662392139434814\n",
      "Loss: 2.5181972980499268\n",
      "Loss: 2.498415470123291\n",
      "Loss: 2.4486405849456787\n",
      "Loss: 2.5216031074523926\n",
      "Loss: 2.3311166763305664\n",
      "Loss: 2.544611692428589\n",
      "Loss: 2.512299060821533\n",
      "Loss: 2.3965020179748535\n",
      "Loss: 2.4652349948883057\n",
      "Loss: 2.4432101249694824\n",
      "Loss: 2.4660544395446777\n",
      "Loss: 2.5113022327423096\n",
      "Loss: 2.437476634979248\n",
      "Loss: 2.3623030185699463\n",
      "Loss: 2.4544930458068848\n",
      "Loss: 2.4613513946533203\n",
      "Loss: 2.443660020828247\n",
      "Loss: 2.4027364253997803\n",
      "Loss: 2.482382297515869\n",
      "Loss: 2.489840030670166\n",
      "Loss: 2.4723281860351562\n",
      "Loss: 2.4553050994873047\n",
      "Loss: 2.4418013095855713\n",
      "Loss: 2.379761219024658\n",
      "Loss: 2.3922038078308105\n",
      "Loss: 2.5004079341888428\n",
      "Loss: 2.4697601795196533\n",
      "Loss: 2.390484571456909\n",
      "Loss: 2.5358221530914307\n",
      "Loss: 2.4520020484924316\n",
      "Loss: 2.4914512634277344\n",
      "Loss: 2.427001953125\n",
      "Loss: 2.5601468086242676\n",
      "Loss: 2.4958300590515137\n",
      "Loss: 2.3977694511413574\n",
      "Loss: 2.417635440826416\n",
      "Loss: 2.526994228363037\n",
      "Loss: 2.4272496700286865\n",
      "Loss: 2.359990119934082\n",
      "Loss: 2.5362448692321777\n",
      "Loss: 2.516335964202881\n",
      "Loss: 2.476087808609009\n",
      "Loss: 2.475639820098877\n",
      "Loss: 2.445892810821533\n",
      "Loss: 2.4831552505493164\n",
      "Loss: 2.4181485176086426\n",
      "Loss: 2.429673671722412\n",
      "Loss: 2.4198296070098877\n",
      "Loss: 2.431903600692749\n",
      "Loss: 2.3369016647338867\n",
      "Loss: 2.414754867553711\n",
      "Loss: 2.46458101272583\n",
      "Loss: 2.3817076683044434\n",
      "Loss: 2.445858955383301\n",
      "Loss: 2.4626564979553223\n",
      "Loss: 2.404716968536377\n",
      "Loss: 2.401575803756714\n",
      "Loss: 2.4155073165893555\n",
      "Loss: 2.401827335357666\n",
      "Loss: 2.480320453643799\n",
      "Loss: 2.4019625186920166\n",
      "Loss: 2.489259719848633\n",
      "Loss: 2.3500359058380127\n",
      "Loss: 2.443540096282959\n",
      "Loss: 2.43601393699646\n",
      "Loss: 2.435305118560791\n",
      "Loss: 2.290699005126953\n",
      "Loss: 2.4051926136016846\n",
      "Loss: 2.411182403564453\n",
      "Loss: 2.4512767791748047\n",
      "Loss: 2.3885953426361084\n",
      "Loss: 2.5548481941223145\n",
      "Loss: 2.4385454654693604\n",
      "Loss: 2.4171900749206543\n",
      "Loss: 2.3730087280273438\n",
      "Loss: 2.504769802093506\n",
      "Loss: 2.461474895477295\n",
      "Loss: 2.3546624183654785\n",
      "Loss: 2.412785530090332\n",
      "Loss: 2.507814884185791\n",
      "Loss: 2.4141149520874023\n",
      "Loss: 2.411979913711548\n",
      "Loss: 2.408323049545288\n",
      "Loss: 2.3689961433410645\n",
      "Loss: 2.462489604949951\n",
      "Loss: 2.5312023162841797\n",
      "Loss: 2.3288369178771973\n",
      "Loss: 2.330414056777954\n",
      "Loss: 2.3539628982543945\n",
      "Loss: 2.3672237396240234\n",
      "Loss: 2.4473133087158203\n",
      "Loss: 2.369419574737549\n",
      "Loss: 2.3936524391174316\n",
      "Loss: 2.4002254009246826\n",
      "Loss: 2.4299814701080322\n",
      "Loss: 2.4033126831054688\n",
      "Loss: 2.3942043781280518\n",
      "Loss: 2.2924160957336426\n",
      "Loss: 2.402827262878418\n",
      "Loss: 2.292576789855957\n",
      "Loss: 2.3035683631896973\n",
      "Loss: 2.282867431640625\n",
      "Loss: 2.370290756225586\n",
      "Loss: 2.3253345489501953\n",
      "Loss: 2.4319908618927\n",
      "Loss: 2.278517723083496\n",
      "Loss: 2.349335193634033\n",
      "Loss: 2.3048582077026367\n",
      "Loss: 2.339576244354248\n",
      "Loss: 2.3082499504089355\n",
      "Loss: 2.3474326133728027\n",
      "Loss: 2.262699604034424\n",
      "Loss: 2.295572280883789\n",
      "Loss: 2.3974392414093018\n",
      "Loss: 2.3531970977783203\n",
      "Loss: 2.2687745094299316\n",
      "Loss: 2.3443045616149902\n",
      "Loss: 2.3395438194274902\n",
      "Loss: 2.254748821258545\n",
      "Loss: 2.258411169052124\n",
      "Loss: 2.358173370361328\n",
      "Loss: 2.321056604385376\n",
      "Loss: 2.249706745147705\n",
      "Loss: 2.4053707122802734\n",
      "Loss: 2.350064277648926\n",
      "Loss: 2.335148334503174\n",
      "Loss: 2.257758617401123\n",
      "Loss: 2.3992507457733154\n",
      "Loss: 2.324310779571533\n",
      "Loss: 2.3899266719818115\n",
      "Loss: 2.3178296089172363\n",
      "Loss: 2.2544174194335938\n",
      "Loss: 2.2872300148010254\n",
      "Loss: 2.29256010055542\n",
      "Loss: 2.2746450901031494\n",
      "Loss: 2.324660301208496\n",
      "Loss: 2.3321540355682373\n",
      "Loss: 2.242194890975952\n",
      "Loss: 2.2798614501953125\n",
      "Loss: 2.3221676349639893\n",
      "Loss: 2.298647880554199\n",
      "Loss: 2.293126344680786\n",
      "Loss: 2.3821585178375244\n",
      "Loss: 2.2810587882995605\n",
      "Loss: 2.294764757156372\n",
      "Loss: 2.3550612926483154\n",
      "Loss: 2.392507791519165\n",
      "Loss: 2.4589755535125732\n",
      "Loss: 2.26143479347229\n",
      "Loss: 2.2863104343414307\n",
      "Loss: 2.3280978202819824\n",
      "Loss: 2.3164892196655273\n",
      "Loss: 2.295780658721924\n",
      "Loss: 2.3431835174560547\n",
      "Loss: 2.2559266090393066\n",
      "Loss: 2.348336696624756\n",
      "Loss: 2.270749568939209\n",
      "Loss: 2.4711647033691406\n",
      "Loss: 2.275157928466797\n",
      "Loss: 2.2692995071411133\n",
      "Loss: 2.2718982696533203\n",
      "Loss: 2.244419574737549\n",
      "Loss: 2.2459025382995605\n",
      "Loss: 2.291442394256592\n",
      "Loss: 2.3476107120513916\n",
      "Loss: 2.4216504096984863\n",
      "Loss: 2.4199209213256836\n",
      "Loss: 2.373218297958374\n",
      "Loss: 2.282731294631958\n",
      "Loss: 2.3642401695251465\n",
      "Loss: 2.2956666946411133\n",
      "Loss: 2.3557958602905273\n",
      "Loss: 2.298297166824341\n",
      "Loss: 2.2429356575012207\n",
      "Loss: 2.291586399078369\n",
      "Loss: 2.310241937637329\n",
      "Loss: 2.311121940612793\n",
      "Loss: 2.331819534301758\n",
      "Loss: 2.2812538146972656\n",
      "Loss: 2.2431154251098633\n",
      "Loss: 2.2617363929748535\n",
      "Loss: 2.291196823120117\n",
      "Loss: 2.2736878395080566\n",
      "Loss: 2.2294232845306396\n",
      "Loss: 2.299661159515381\n",
      "Loss: 2.305555820465088\n",
      "Loss: 2.249422788619995\n",
      "Loss: 2.293970823287964\n",
      "Loss: 2.269958734512329\n",
      "Loss: 2.3258559703826904\n",
      "Loss: 2.318965196609497\n",
      "Loss: 2.333592176437378\n",
      "Loss: 2.289107322692871\n",
      "Loss: 2.299950122833252\n",
      "Loss: 2.3228631019592285\n",
      "Loss: 2.356051445007324\n",
      "Loss: 2.2467103004455566\n",
      "Loss: 2.270120620727539\n",
      "Loss: 2.2349328994750977\n",
      "Loss: 2.3466086387634277\n",
      "Loss: 2.2880148887634277\n",
      "Loss: 2.2991464138031006\n",
      "Loss: 2.3026185035705566\n",
      "Loss: 2.2827179431915283\n",
      "Loss: 2.29536509513855\n",
      "Loss: 2.3202853202819824\n",
      "Loss: 2.2666940689086914\n",
      "Loss: 2.278371810913086\n",
      "Loss: 2.2854015827178955\n",
      "Loss: 2.303562641143799\n",
      "Loss: 2.3175415992736816\n",
      "Loss: 2.399583578109741\n",
      "Loss: 2.237412691116333\n",
      "Loss: 2.287031650543213\n",
      "Loss: 2.338658571243286\n",
      "Loss: 2.3295435905456543\n",
      "Loss: 2.3188931941986084\n",
      "Loss: 2.330134868621826\n",
      "Loss: 2.2339978218078613\n",
      "Loss: 2.2458741664886475\n",
      "Loss: 2.25526762008667\n",
      "Loss: 2.2610716819763184\n",
      "Loss: 2.3543810844421387\n",
      "Loss: 2.2941060066223145\n",
      "Loss: 2.2776095867156982\n",
      "Loss: 2.2398452758789062\n",
      "Loss: 2.3606319427490234\n",
      "Loss: 2.416289806365967\n",
      "Loss: 2.242250680923462\n",
      "Loss: 2.28292179107666\n",
      "Loss: 2.311187982559204\n",
      "Loss: 2.2405431270599365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3124542236328125\n",
      "Loss: 2.2162041664123535\n",
      "Loss: 2.353451728820801\n",
      "Loss: 2.286012649536133\n",
      "Loss: 2.32828950881958\n",
      "Loss: 2.2670934200286865\n",
      "Loss: 2.3116707801818848\n",
      "Loss: 2.309291124343872\n",
      "Loss: 2.328505039215088\n",
      "Loss: 2.278268814086914\n",
      "Loss: 2.2525854110717773\n",
      "Loss: 2.275333881378174\n",
      "Loss: 2.3100814819335938\n",
      "Loss: 2.2996444702148438\n",
      "Loss: 2.354173421859741\n",
      "Loss: 2.2831077575683594\n",
      "Loss: 2.3161709308624268\n",
      "Loss: 2.293020248413086\n",
      "Loss: 2.3363406658172607\n",
      "Loss: 2.2812561988830566\n",
      "Loss: 2.307560443878174\n",
      "Loss: 2.3157012462615967\n",
      "Loss: 2.2569470405578613\n",
      "Loss: 2.303253650665283\n",
      "Loss: 2.230196952819824\n",
      "Loss: 2.2931013107299805\n",
      "Loss: 2.2403550148010254\n",
      "Loss: 2.250673770904541\n",
      "Loss: 2.319026470184326\n",
      "Loss: 2.3043861389160156\n",
      "Loss: 2.2663683891296387\n",
      "Loss: 2.245804786682129\n",
      "Loss: 2.322495937347412\n",
      "Loss: 2.2858872413635254\n",
      "Loss: 2.3568577766418457\n",
      "Loss: 2.2878496646881104\n",
      "Loss: 2.2512478828430176\n",
      "Loss: 2.2459731101989746\n",
      "Loss: 2.274606704711914\n",
      "Loss: 2.204158067703247\n",
      "Loss: 2.3238587379455566\n",
      "Loss: 2.2145438194274902\n",
      "Loss: 2.204294443130493\n",
      "Loss: 2.254814624786377\n",
      "Loss: 2.210265636444092\n",
      "Loss: 2.2363810539245605\n",
      "Loss: 2.2345032691955566\n",
      "Loss: 2.236476421356201\n",
      "Loss: 2.282163619995117\n",
      "Loss: 2.271681308746338\n",
      "Loss: 2.2116079330444336\n",
      "Loss: 2.1899261474609375\n",
      "Loss: 2.2283267974853516\n",
      "Loss: 2.1869959831237793\n",
      "Loss: 2.237031936645508\n",
      "Loss: 2.1371612548828125\n",
      "Loss: 2.146106004714966\n",
      "Loss: 2.3058559894561768\n",
      "Loss: 2.255185127258301\n",
      "Loss: 2.231618881225586\n",
      "Loss: 2.175814390182495\n",
      "Loss: 2.2879066467285156\n",
      "Loss: 2.260638475418091\n",
      "Loss: 2.214114189147949\n",
      "Loss: 2.2749931812286377\n",
      "Loss: 2.245272159576416\n",
      "Loss: 2.206540584564209\n",
      "Loss: 2.1854729652404785\n",
      "Loss: 2.259563446044922\n",
      "Loss: 2.2798960208892822\n",
      "Loss: 2.176912784576416\n",
      "Loss: 2.145803213119507\n",
      "Loss: 2.164677143096924\n",
      "Loss: 2.178353786468506\n",
      "Loss: 2.1670475006103516\n",
      "Loss: 2.143782615661621\n",
      "Loss: 2.215435028076172\n",
      "Loss: 2.2297019958496094\n",
      "Loss: 2.2644004821777344\n",
      "Loss: 2.208376884460449\n",
      "Loss: 2.2018990516662598\n",
      "Loss: 2.2193984985351562\n",
      "Loss: 2.286264181137085\n",
      "Loss: 2.284385919570923\n",
      "Loss: 2.1977105140686035\n",
      "Loss: 2.185296058654785\n",
      "Loss: 2.238509178161621\n",
      "Loss: 2.274559497833252\n",
      "Loss: 2.178572177886963\n",
      "Loss: 2.2044386863708496\n",
      "Loss: 2.159514904022217\n",
      "Loss: 2.2082927227020264\n",
      "Loss: 2.178157329559326\n",
      "Loss: 2.237137794494629\n",
      "Loss: 2.1986351013183594\n",
      "Loss: 2.21805477142334\n",
      "Loss: 2.193420171737671\n",
      "Loss: 2.177337169647217\n",
      "Loss: 2.2012572288513184\n",
      "Loss: 2.222104787826538\n",
      "Loss: 2.1727428436279297\n",
      "Loss: 2.230764389038086\n",
      "Loss: 2.179391384124756\n",
      "Loss: 2.1646392345428467\n",
      "Loss: 2.2257680892944336\n",
      "Loss: 2.2181196212768555\n",
      "Loss: 2.2155842781066895\n",
      "Loss: 2.2246408462524414\n",
      "Loss: 2.2477359771728516\n",
      "Loss: 2.2239646911621094\n",
      "Loss: 2.275895833969116\n",
      "Loss: 2.2065701484680176\n",
      "Loss: 2.196014642715454\n",
      "Loss: 2.2322373390197754\n",
      "Loss: 2.186788558959961\n",
      "Loss: 2.199672222137451\n",
      "Loss: 2.2293343544006348\n",
      "Loss: 2.191004991531372\n",
      "Loss: 2.186366081237793\n",
      "Loss: 2.204359292984009\n",
      "Loss: 2.2269439697265625\n",
      "Loss: 2.2285633087158203\n",
      "Loss: 2.301443099975586\n",
      "Loss: 2.2595841884613037\n",
      "Loss: 2.157288074493408\n",
      "Loss: 2.2077150344848633\n",
      "Loss: 2.1946804523468018\n",
      "Loss: 2.1930148601531982\n",
      "Loss: 2.2408194541931152\n",
      "Loss: 2.197293281555176\n",
      "Loss: 2.1582579612731934\n",
      "Loss: 2.1884093284606934\n",
      "Loss: 2.1637606620788574\n",
      "Loss: 2.2328662872314453\n",
      "Loss: 2.2543134689331055\n",
      "Loss: 2.233044147491455\n",
      "Loss: 2.3349618911743164\n",
      "Loss: 2.1734559535980225\n",
      "Loss: 2.18609619140625\n",
      "Loss: 2.207143783569336\n",
      "Loss: 2.228534460067749\n",
      "Loss: 2.1991944313049316\n",
      "Loss: 2.1839394569396973\n",
      "Loss: 2.2470836639404297\n",
      "Loss: 2.2739405632019043\n",
      "Loss: 2.247276782989502\n",
      "Loss: 2.278991222381592\n",
      "Loss: 2.191120147705078\n",
      "Loss: 2.189000129699707\n",
      "Loss: 2.2611911296844482\n",
      "Loss: 2.2163515090942383\n",
      "Loss: 2.166193962097168\n",
      "Loss: 2.171247959136963\n",
      "Loss: 2.214465379714966\n",
      "Loss: 2.191770553588867\n",
      "Loss: 2.126248836517334\n",
      "Loss: 2.244523525238037\n",
      "Loss: 2.181881904602051\n",
      "Loss: 2.2057251930236816\n",
      "Loss: 2.1986982822418213\n",
      "Loss: 2.2390427589416504\n",
      "Loss: 2.1990935802459717\n",
      "Loss: 2.2437496185302734\n",
      "Loss: 2.255049705505371\n",
      "Loss: 2.1772851943969727\n",
      "Loss: 2.251300811767578\n",
      "Loss: 2.188000202178955\n",
      "Loss: 2.1907095909118652\n",
      "Loss: 2.1744704246520996\n",
      "Loss: 2.2453701496124268\n",
      "Loss: 2.178290843963623\n",
      "Loss: 2.2077476978302\n",
      "Loss: 2.1933767795562744\n",
      "Loss: 2.1698875427246094\n",
      "Loss: 2.243577241897583\n",
      "Loss: 2.16654109954834\n",
      "Loss: 2.2207043170928955\n",
      "Loss: 2.2041242122650146\n",
      "Loss: 2.3137049674987793\n",
      "Loss: 2.181058883666992\n",
      "Loss: 2.16481614112854\n",
      "Loss: 2.1753933429718018\n",
      "Loss: 2.1644885540008545\n",
      "Loss: 2.2100586891174316\n",
      "Loss: 2.2223174571990967\n",
      "Loss: 2.1718740463256836\n",
      "Loss: 2.1442012786865234\n",
      "Loss: 2.23059344291687\n",
      "Loss: 2.2377216815948486\n",
      "Loss: 2.1571083068847656\n",
      "Loss: 2.194403886795044\n",
      "Loss: 2.230318546295166\n",
      "Loss: 2.2264504432678223\n",
      "Loss: 2.1585137844085693\n",
      "Loss: 2.1904456615448\n",
      "Loss: 2.165236473083496\n",
      "Loss: 2.2009811401367188\n",
      "Loss: 2.214977741241455\n",
      "Loss: 2.216865301132202\n",
      "Loss: 2.172534465789795\n",
      "Loss: 2.2031781673431396\n",
      "Loss: 2.1875691413879395\n",
      "Loss: 2.2019994258880615\n",
      "Loss: 2.1933798789978027\n",
      "Loss: 2.219633102416992\n",
      "Loss: 2.1714086532592773\n",
      "Loss: 2.1740241050720215\n",
      "Loss: 2.2256009578704834\n",
      "Loss: 2.173030376434326\n",
      "Loss: 2.186969757080078\n",
      "Loss: 2.165877103805542\n",
      "Loss: 2.2737584114074707\n",
      "Loss: 2.199868679046631\n",
      "Loss: 2.2118659019470215\n",
      "Loss: 2.1824777126312256\n",
      "Loss: 2.242659091949463\n",
      "Loss: 2.212960958480835\n",
      "Loss: 2.202385902404785\n",
      "Loss: 2.2220335006713867\n",
      "Loss: 2.1532604694366455\n",
      "Loss: 2.179011821746826\n",
      "Loss: 2.2228126525878906\n",
      "Loss: 2.1791939735412598\n",
      "Loss: 2.1968746185302734\n",
      "Loss: 2.182447910308838\n",
      "Loss: 2.196592330932617\n",
      "Loss: 2.200162649154663\n",
      "Loss: 2.170480966567993\n",
      "Loss: 2.2376224994659424\n",
      "Loss: 2.17379093170166\n",
      "Loss: 2.1739273071289062\n",
      "Loss: 2.181555986404419\n",
      "Loss: 2.1616575717926025\n",
      "Loss: 2.1669187545776367\n",
      "Loss: 2.1373322010040283\n",
      "Loss: 2.160407304763794\n",
      "Loss: 2.2180278301239014\n",
      "Loss: 2.179434061050415\n",
      "Loss: 2.162461042404175\n",
      "Loss: 2.2210941314697266\n",
      "Loss: 2.1009554862976074\n",
      "Loss: 2.1615209579467773\n",
      "Loss: 2.2348575592041016\n",
      "Loss: 2.1819794178009033\n",
      "Loss: 2.134086847305298\n",
      "Loss: 2.137580394744873\n",
      "Loss: 2.241837501525879\n",
      "Loss: 2.2058565616607666\n",
      "Loss: 2.191157341003418\n",
      "Loss: 2.2167611122131348\n",
      "Loss: 2.2031874656677246\n",
      "Loss: 2.1766576766967773\n",
      "Loss: 2.1769185066223145\n",
      "Loss: 2.165862560272217\n",
      "Loss: 2.146259307861328\n",
      "Loss: 2.1774187088012695\n",
      "Loss: 2.154994249343872\n",
      "Loss: 2.168382167816162\n",
      "Loss: 2.1520590782165527\n",
      "Loss: 2.1500377655029297\n",
      "Loss: 2.168496608734131\n",
      "Loss: 2.219097375869751\n",
      "Loss: 2.16330623626709\n",
      "Loss: 2.1868410110473633\n",
      "Loss: 2.1746323108673096\n",
      "Loss: 2.158889055252075\n",
      "Loss: 2.1909027099609375\n",
      "Loss: 2.2390058040618896\n",
      "Loss: 2.1380040645599365\n",
      "Loss: 2.203092098236084\n",
      "Loss: 2.1149983406066895\n",
      "Loss: 2.199963092803955\n",
      "Loss: 2.1629838943481445\n",
      "Loss: 2.1883749961853027\n",
      "Loss: 2.153212070465088\n",
      "Loss: 2.1777377128601074\n",
      "Loss: 2.1635043621063232\n",
      "Loss: 2.1769487857818604\n",
      "Loss: 2.1603167057037354\n",
      "Loss: 2.271684408187866\n",
      "Loss: 2.1179141998291016\n",
      "Loss: 2.161527156829834\n",
      "Loss: 2.191230297088623\n",
      "Loss: 2.165459632873535\n",
      "Loss: 2.1794025897979736\n",
      "Loss: 2.147869348526001\n",
      "Loss: 2.2552130222320557\n",
      "Loss: 2.1860623359680176\n",
      "Loss: 2.216427803039551\n",
      "Loss: 2.173527479171753\n",
      "Loss: 2.2038283348083496\n",
      "Loss: 2.14599609375\n",
      "Loss: 2.164766311645508\n",
      "Loss: 2.160551071166992\n",
      "Loss: 2.1814894676208496\n",
      "Loss: 2.129720687866211\n",
      "Loss: 2.1640989780426025\n",
      "Loss: 2.1378374099731445\n",
      "Loss: 2.165069103240967\n",
      "Loss: 2.105879783630371\n",
      "Loss: 2.1904032230377197\n",
      "Loss: 2.225395917892456\n",
      "Loss: 2.180413246154785\n",
      "Loss: 2.1992547512054443\n",
      "Loss: 2.180753231048584\n",
      "Loss: 2.189124584197998\n",
      "Loss: 2.174323558807373\n",
      "Loss: 2.1595420837402344\n",
      "Loss: 2.1594033241271973\n",
      "Loss: 2.204956531524658\n",
      "Loss: 2.177950382232666\n",
      "Loss: 2.201899290084839\n",
      "Loss: 2.181122303009033\n",
      "Loss: 2.201042652130127\n",
      "Loss: 2.1368536949157715\n",
      "Loss: 2.172565460205078\n",
      "Loss: 2.164188861846924\n",
      "Loss: 2.207606077194214\n",
      "Loss: 2.219099998474121\n",
      "Loss: 2.1748218536376953\n",
      "Loss: 2.143897533416748\n",
      "Loss: 2.123958110809326\n",
      "Loss: 2.144425392150879\n",
      "Loss: 2.1736416816711426\n",
      "Loss: 2.2024312019348145\n",
      "Loss: 2.221907377243042\n",
      "Loss: 2.195136785507202\n",
      "Loss: 2.1105005741119385\n",
      "Loss: 2.1603634357452393\n",
      "Loss: 2.1168508529663086\n",
      "Loss: 2.14262056350708\n",
      "Loss: 2.188652992248535\n",
      "Loss: 2.1719696521759033\n",
      "Loss: 2.129347324371338\n",
      "Loss: 2.176363945007324\n",
      "Loss: 2.1918118000030518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.1814393997192383\n",
      "Loss: 2.2104382514953613\n",
      "Loss: 2.1716177463531494\n",
      "Loss: 2.1148204803466797\n",
      "Loss: 2.1787030696868896\n",
      "Loss: 2.192112922668457\n",
      "Loss: 2.1681885719299316\n",
      "Loss: 2.1879916191101074\n",
      "Loss: 2.2115073204040527\n",
      "Loss: 2.174830913543701\n",
      "Loss: 2.148698329925537\n",
      "Loss: 2.1949462890625\n",
      "Loss: 2.179013967514038\n",
      "Loss: 2.174501895904541\n",
      "Loss: 2.1277108192443848\n",
      "Loss: 2.2325587272644043\n",
      "Loss: 2.167574405670166\n",
      "Loss: 2.201584815979004\n",
      "Loss: 2.168069839477539\n",
      "Loss: 2.2016844749450684\n",
      "Loss: 2.125929355621338\n",
      "Loss: 2.1760261058807373\n",
      "Loss: 2.199984550476074\n",
      "Loss: 2.1852030754089355\n",
      "Loss: 2.166743040084839\n",
      "Loss: 2.2013323307037354\n",
      "Loss: 2.1631596088409424\n",
      "Loss: 2.1388673782348633\n",
      "Loss: 2.1233088970184326\n",
      "Loss: 2.2328882217407227\n",
      "Loss: 2.174056053161621\n",
      "Loss: 2.128298044204712\n",
      "Loss: 2.118079662322998\n",
      "Loss: 2.2042081356048584\n",
      "Loss: 2.165684223175049\n",
      "Loss: 2.161465883255005\n",
      "Loss: 2.2057247161865234\n",
      "Loss: 2.1711509227752686\n",
      "Loss: 2.193124771118164\n",
      "Loss: 2.14871883392334\n",
      "Loss: 2.143306255340576\n",
      "Loss: 2.1735215187072754\n",
      "Loss: 2.147144317626953\n",
      "Loss: 2.2041773796081543\n",
      "Loss: 2.1322460174560547\n",
      "Loss: 2.1753673553466797\n",
      "Loss: 2.1944022178649902\n",
      "Loss: 2.184626340866089\n",
      "Loss: 2.280587673187256\n",
      "Loss: 2.1947522163391113\n",
      "Loss: 2.143080472946167\n",
      "Loss: 2.2403512001037598\n",
      "Loss: 2.1687703132629395\n",
      "Loss: 2.2011468410491943\n",
      "Loss: 2.1585373878479004\n",
      "Loss: 2.1764602661132812\n",
      "Loss: 2.1275784969329834\n",
      "Loss: 2.197411298751831\n",
      "Loss: 2.160451889038086\n",
      "Loss: 2.1893694400787354\n",
      "Loss: 2.15763521194458\n",
      "Loss: 2.1449596881866455\n",
      "Loss: 2.2107369899749756\n",
      "Loss: 2.229443073272705\n",
      "Loss: 2.178309917449951\n",
      "Loss: 2.191382884979248\n",
      "Loss: 2.127943515777588\n",
      "Loss: 2.1256442070007324\n",
      "Loss: 2.1107430458068848\n",
      "Loss: 2.1766624450683594\n",
      "Loss: 2.170058250427246\n",
      "Loss: 2.1718225479125977\n",
      "Loss: 2.158262252807617\n",
      "Loss: 2.169745922088623\n",
      "Loss: 2.189304828643799\n",
      "Loss: 2.2230072021484375\n",
      "Loss: 2.1204581260681152\n",
      "Loss: 2.2088184356689453\n",
      "Loss: 2.173143148422241\n",
      "Loss: 2.2347536087036133\n",
      "Loss: 2.1796979904174805\n",
      "Loss: 2.158271551132202\n",
      "Loss: 2.2011852264404297\n",
      "Loss: 2.142280101776123\n",
      "Loss: 2.1689250469207764\n",
      "Loss: 2.1428725719451904\n",
      "Loss: 2.202904224395752\n",
      "Loss: 2.169670581817627\n",
      "Loss: 2.163414478302002\n",
      "Loss: 2.2292890548706055\n",
      "Loss: 2.149507522583008\n",
      "Loss: 2.188098669052124\n",
      "Loss: 2.177794933319092\n",
      "Loss: 2.169811964035034\n",
      "Loss: 2.1724889278411865\n",
      "Loss: 2.124974489212036\n",
      "Loss: 2.183551788330078\n",
      "Loss: 2.145322799682617\n",
      "Loss: 2.1276845932006836\n",
      "Loss: 2.159245491027832\n",
      "Loss: 2.17702054977417\n",
      "Loss: 2.1775870323181152\n",
      "Loss: 2.128458023071289\n",
      "Loss: 2.1442298889160156\n",
      "Loss: 2.142971992492676\n",
      "Loss: 2.1364312171936035\n",
      "Loss: 2.130654811859131\n",
      "Loss: 2.2033257484436035\n",
      "Loss: 2.188486337661743\n",
      "Loss: 2.2030582427978516\n",
      "Loss: 2.159409999847412\n",
      "Loss: 2.1423609256744385\n",
      "Loss: 2.12683367729187\n",
      "Loss: 2.1307761669158936\n",
      "Loss: 2.1332077980041504\n",
      "Loss: 2.127624034881592\n",
      "Loss: 2.1891870498657227\n",
      "Loss: 2.1375951766967773\n",
      "Loss: 2.1572916507720947\n",
      "Loss: 2.172736644744873\n",
      "Loss: 2.159909248352051\n",
      "Loss: 2.142519235610962\n",
      "Loss: 2.143111228942871\n",
      "Loss: 2.1662206649780273\n",
      "Loss: 2.1406753063201904\n",
      "Loss: 2.1599605083465576\n",
      "Loss: 2.172877788543701\n",
      "Loss: 2.1746983528137207\n",
      "Loss: 2.219569444656372\n",
      "Loss: 2.166577100753784\n",
      "Loss: 2.14555025100708\n",
      "Loss: 2.1427459716796875\n",
      "Loss: 2.171670913696289\n",
      "Loss: 2.1580348014831543\n",
      "Loss: 2.1895885467529297\n",
      "Loss: 2.1441335678100586\n",
      "Loss: 2.1001949310302734\n",
      "Loss: 2.1879501342773438\n",
      "Loss: 2.172379970550537\n",
      "Loss: 2.142584800720215\n",
      "Loss: 2.114682674407959\n",
      "Loss: 2.203423261642456\n",
      "Loss: 2.127258062362671\n",
      "Loss: 2.1756322383880615\n",
      "Loss: 2.1988840103149414\n",
      "Loss: 2.1427674293518066\n",
      "Loss: 2.0963478088378906\n",
      "Loss: 2.151360511779785\n",
      "Loss: 2.158963203430176\n",
      "Loss: 2.1422481536865234\n",
      "Loss: 2.1715033054351807\n",
      "Loss: 2.1573054790496826\n",
      "Loss: 2.1701579093933105\n",
      "Loss: 2.112748861312866\n",
      "Loss: 2.17218017578125\n",
      "Loss: 2.187403678894043\n",
      "Loss: 2.1573917865753174\n",
      "Loss: 2.1811718940734863\n",
      "Loss: 2.173020362854004\n",
      "Loss: 2.162654399871826\n",
      "Loss: 2.1566617488861084\n",
      "Loss: 2.1902036666870117\n",
      "Loss: 2.156247138977051\n",
      "Loss: 2.126098155975342\n",
      "Loss: 2.112431526184082\n",
      "Loss: 2.143582820892334\n",
      "Loss: 2.1304829120635986\n",
      "Loss: 2.1278581619262695\n",
      "Loss: 2.1881263256073\n",
      "Loss: 2.1309666633605957\n",
      "Loss: 2.248288154602051\n",
      "Loss: 2.1433863639831543\n",
      "Loss: 2.220001459121704\n",
      "Loss: 2.1733977794647217\n",
      "Loss: 2.1661343574523926\n",
      "Loss: 2.1571788787841797\n",
      "Loss: 2.1273345947265625\n",
      "Loss: 2.15104341506958\n",
      "Loss: 2.208888053894043\n",
      "Loss: 2.2021729946136475\n",
      "Loss: 2.1438612937927246\n",
      "Loss: 2.187655210494995\n",
      "Loss: 2.155677318572998\n",
      "Loss: 2.2030630111694336\n",
      "Loss: 2.170992851257324\n",
      "Loss: 2.2443132400512695\n",
      "Loss: 2.156754970550537\n",
      "Loss: 2.1266050338745117\n",
      "Loss: 2.1600089073181152\n",
      "Loss: 2.1426334381103516\n",
      "Loss: 2.171891689300537\n",
      "Loss: 2.116002321243286\n",
      "Loss: 2.1417438983917236\n",
      "Loss: 2.1565768718719482\n",
      "Loss: 2.1870853900909424\n",
      "Loss: 2.1881654262542725\n",
      "Loss: 2.1709342002868652\n",
      "Loss: 2.1868066787719727\n",
      "Loss: 2.162997245788574\n",
      "Loss: 2.1871509552001953\n",
      "Loss: 2.129136323928833\n",
      "Loss: 2.1797218322753906\n",
      "Loss: 2.095141887664795\n",
      "Loss: 2.114229679107666\n",
      "Loss: 2.172663927078247\n",
      "Loss: 2.1518192291259766\n",
      "Loss: 2.196629047393799\n",
      "Loss: 2.2324278354644775\n",
      "Loss: 2.12715220451355\n",
      "Loss: 2.1411120891571045\n",
      "Loss: 2.1577906608581543\n",
      "Loss: 2.1427559852600098\n",
      "Loss: 2.1448192596435547\n",
      "Loss: 2.1909308433532715\n",
      "Loss: 2.1515867710113525\n",
      "Loss: 2.171816349029541\n",
      "Loss: 2.186969757080078\n",
      "Loss: 2.203469753265381\n",
      "Loss: 2.200526237487793\n",
      "Loss: 2.159398078918457\n",
      "Loss: 2.1668362617492676\n",
      "Loss: 2.17214298248291\n",
      "Loss: 2.144411563873291\n",
      "Loss: 2.1112051010131836\n",
      "Loss: 2.23110032081604\n",
      "Loss: 2.2336537837982178\n",
      "Loss: 2.205554723739624\n",
      "Loss: 2.1733570098876953\n",
      "Loss: 2.188185691833496\n",
      "Loss: 2.113220691680908\n",
      "Loss: 2.1801676750183105\n",
      "Loss: 2.1867589950561523\n",
      "Loss: 2.1717686653137207\n",
      "Loss: 2.202303409576416\n",
      "Loss: 2.188394784927368\n",
      "Loss: 2.177333116531372\n",
      "Loss: 2.172579288482666\n",
      "Loss: 2.141064405441284\n",
      "Loss: 2.1559195518493652\n",
      "Loss: 2.1413397789001465\n",
      "Loss: 2.1418826580047607\n",
      "Loss: 2.2013955116271973\n",
      "Loss: 2.1265645027160645\n",
      "Loss: 2.1722211837768555\n",
      "Loss: 2.1414687633514404\n",
      "Loss: 2.140862464904785\n",
      "Loss: 2.187183380126953\n",
      "Loss: 2.1712679862976074\n",
      "Loss: 2.1414973735809326\n",
      "Loss: 2.156336545944214\n",
      "Loss: 2.1560449600219727\n",
      "Loss: 2.172631025314331\n",
      "Loss: 2.1404776573181152\n",
      "Loss: 2.171435832977295\n",
      "Loss: 2.1256613731384277\n",
      "Loss: 2.1561670303344727\n",
      "Loss: 2.141343116760254\n",
      "Loss: 2.1249990463256836\n",
      "Loss: 2.156222343444824\n",
      "Loss: 2.1553072929382324\n",
      "Loss: 2.1550328731536865\n",
      "Loss: 2.1567654609680176\n",
      "Loss: 2.1717734336853027\n",
      "Loss: 2.1711840629577637\n",
      "Loss: 2.140646457672119\n",
      "Loss: 2.1585893630981445\n",
      "Loss: 2.1860604286193848\n",
      "Loss: 2.1716907024383545\n",
      "Loss: 2.20536470413208\n",
      "Loss: 2.140925407409668\n",
      "Loss: 2.125734567642212\n",
      "Loss: 2.1859006881713867\n",
      "Loss: 2.1712486743927\n",
      "Loss: 2.2603116035461426\n",
      "Loss: 2.1256203651428223\n",
      "Loss: 2.156797409057617\n",
      "Loss: 2.1720662117004395\n",
      "Loss: 2.1296467781066895\n",
      "Loss: 2.156191349029541\n",
      "Loss: 2.2570595741271973\n",
      "Loss: 2.1412839889526367\n",
      "Loss: 2.1107821464538574\n",
      "Loss: 2.1858575344085693\n",
      "Loss: 2.112189531326294\n",
      "Loss: 2.126357316970825\n",
      "Loss: 2.1259822845458984\n",
      "Loss: 2.1264543533325195\n",
      "Loss: 2.1421608924865723\n",
      "Loss: 2.1569693088531494\n",
      "Loss: 2.1557445526123047\n",
      "Loss: 2.1309330463409424\n",
      "Loss: 2.2017405033111572\n",
      "Loss: 2.166721820831299\n",
      "Loss: 2.125579595565796\n",
      "Loss: 2.1866307258605957\n",
      "Loss: 2.164029121398926\n",
      "Loss: 2.1569902896881104\n",
      "Loss: 2.1419529914855957\n",
      "Loss: 2.215923309326172\n",
      "Loss: 2.156125068664551\n",
      "Loss: 2.171154022216797\n",
      "Loss: 2.1725881099700928\n",
      "Loss: 2.232085704803467\n",
      "Loss: 2.2016119956970215\n",
      "Loss: 2.2017760276794434\n",
      "Loss: 2.2363905906677246\n",
      "Loss: 2.1257996559143066\n",
      "Loss: 2.201232433319092\n",
      "Loss: 2.110912561416626\n",
      "Loss: 2.2320282459259033\n",
      "Loss: 2.1417794227600098\n",
      "Loss: 2.1560959815979004\n",
      "Loss: 2.150861978530884\n",
      "Loss: 2.138401508331299\n",
      "Loss: 2.1614513397216797\n",
      "Loss: 2.0957412719726562\n",
      "Loss: 2.2182259559631348\n",
      "Loss: 2.1867170333862305\n",
      "Loss: 2.142475128173828\n",
      "Loss: 2.181518793106079\n",
      "Loss: 2.172166347503662\n",
      "Loss: 2.1387252807617188\n",
      "Loss: 2.171579360961914\n",
      "Loss: 2.140299081802368\n",
      "Loss: 2.1410982608795166\n",
      "Loss: 2.1566219329833984\n",
      "Loss: 2.154076099395752\n",
      "Loss: 2.1556360721588135\n",
      "Loss: 2.1860456466674805\n",
      "Loss: 2.172107219696045\n",
      "Loss: 2.1566390991210938\n",
      "Loss: 2.1267507076263428\n",
      "Loss: 2.1883022785186768\n",
      "Loss: 2.216762065887451\n",
      "Loss: 2.126378297805786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.1719560623168945\n",
      "Loss: 2.148836612701416\n",
      "Loss: 2.2010836601257324\n",
      "Loss: 2.110286235809326\n",
      "Loss: 2.1112184524536133\n",
      "Loss: 2.18599796295166\n",
      "Loss: 2.1404175758361816\n",
      "Loss: 2.201394557952881\n",
      "Loss: 2.1565284729003906\n",
      "Loss: 2.1400437355041504\n",
      "Loss: 2.172105550765991\n",
      "Loss: 2.187129020690918\n",
      "Loss: 2.140197515487671\n",
      "Loss: 2.1558632850646973\n",
      "Loss: 2.1865234375\n",
      "Loss: 2.1713733673095703\n",
      "Loss: 2.1716196537017822\n",
      "Loss: 2.1250298023223877\n",
      "Loss: 2.1104698181152344\n",
      "Loss: 2.1572818756103516\n",
      "Loss: 2.17134952545166\n",
      "Loss: 2.140850067138672\n",
      "Loss: 2.2170372009277344\n",
      "Loss: 2.1590065956115723\n",
      "Loss: 2.2013742923736572\n",
      "Loss: 2.145000457763672\n",
      "Loss: 2.1259851455688477\n",
      "Loss: 2.186431646347046\n",
      "Loss: 2.2159955501556396\n",
      "Loss: 2.1403589248657227\n",
      "Loss: 2.15541672706604\n",
      "Loss: 2.2479944229125977\n",
      "Loss: 2.1400797367095947\n",
      "Loss: 2.1414294242858887\n",
      "Loss: 2.09542179107666\n",
      "Loss: 2.1405510902404785\n",
      "Loss: 2.125600576400757\n",
      "Loss: 2.2012367248535156\n",
      "Loss: 2.1866559982299805\n",
      "Loss: 2.1573426723480225\n",
      "Loss: 2.125652551651001\n",
      "Loss: 2.110126256942749\n",
      "Loss: 2.1407203674316406\n",
      "Loss: 2.1258654594421387\n",
      "Loss: 2.140015125274658\n",
      "Loss: 2.1143274307250977\n",
      "Loss: 2.1100239753723145\n",
      "Loss: 2.2017688751220703\n",
      "Loss: 2.140908718109131\n",
      "Loss: 2.1626319885253906\n",
      "Loss: 2.1776297092437744\n",
      "Loss: 2.1268467903137207\n",
      "Loss: 2.2180919647216797\n",
      "Loss: 2.216517448425293\n",
      "Loss: 2.1712026596069336\n",
      "Loss: 2.1855194568634033\n",
      "Loss: 2.1411848068237305\n",
      "Loss: 2.1868696212768555\n",
      "Loss: 2.1402218341827393\n",
      "Loss: 2.1711416244506836\n",
      "Loss: 2.125422954559326\n",
      "Loss: 2.186253786087036\n",
      "Loss: 2.1266136169433594\n",
      "Loss: 2.231358766555786\n",
      "Loss: 2.185911178588867\n",
      "Loss: 2.110546350479126\n",
      "Loss: 2.125469207763672\n",
      "Loss: 2.1254069805145264\n",
      "Loss: 2.2162065505981445\n",
      "Loss: 2.1100142002105713\n",
      "Loss: 2.1251883506774902\n",
      "Loss: 2.2325823307037354\n",
      "Loss: 2.140932559967041\n",
      "Loss: 2.139892816543579\n",
      "Loss: 2.1266841888427734\n",
      "Loss: 2.171121120452881\n",
      "Loss: 2.1416263580322266\n",
      "Loss: 2.1589784622192383\n",
      "Loss: 2.140289068222046\n",
      "Loss: 2.186110496520996\n",
      "Loss: 2.1475985050201416\n",
      "Loss: 2.110161304473877\n",
      "Loss: 2.140113353729248\n",
      "Loss: 2.2004292011260986\n",
      "Loss: 2.1557230949401855\n",
      "Loss: 2.1707253456115723\n",
      "Loss: 2.1701955795288086\n",
      "Loss: 2.125545024871826\n",
      "Loss: 2.1255226135253906\n",
      "Loss: 2.1406426429748535\n",
      "Loss: 2.1100873947143555\n",
      "Loss: 2.1703152656555176\n",
      "Loss: 2.219575881958008\n",
      "Loss: 2.1107401847839355\n",
      "Loss: 2.2012414932250977\n",
      "Loss: 2.12860107421875\n",
      "Loss: 2.1559996604919434\n",
      "Loss: 2.171463966369629\n",
      "Loss: 2.1559369564056396\n",
      "Loss: 2.185809850692749\n",
      "Loss: 2.1558663845062256\n",
      "Loss: 2.1863341331481934\n",
      "Loss: 2.1406545639038086\n",
      "Loss: 2.0947370529174805\n",
      "Loss: 2.216238021850586\n",
      "Loss: 2.1860592365264893\n",
      "Loss: 2.215806245803833\n",
      "Loss: 2.1710054874420166\n",
      "Loss: 2.1105732917785645\n",
      "Loss: 2.200719118118286\n",
      "Loss: 2.0797276496887207\n",
      "Loss: 2.1536402702331543\n",
      "Loss: 2.176825523376465\n",
      "Loss: 2.1556477546691895\n",
      "Loss: 2.1547305583953857\n",
      "Loss: 2.094994306564331\n",
      "Loss: 2.1848607063293457\n",
      "Loss: 2.2399394512176514\n",
      "Loss: 2.1548726558685303\n",
      "Loss: 2.140840530395508\n",
      "Loss: 2.1692728996276855\n",
      "Loss: 2.0924060344696045\n",
      "Loss: 2.1244957447052\n",
      "Loss: 2.1101419925689697\n",
      "Loss: 2.14255952835083\n",
      "Loss: 2.1490883827209473\n",
      "Loss: 2.1102232933044434\n",
      "Loss: 2.139016628265381\n",
      "Loss: 2.165039300918579\n",
      "Loss: 2.1450934410095215\n",
      "Loss: 2.1090314388275146\n",
      "Loss: 2.1220757961273193\n",
      "Loss: 2.091430425643921\n",
      "Loss: 2.110477924346924\n",
      "Loss: 2.1409695148468018\n",
      "Loss: 2.1498732566833496\n",
      "Loss: 2.112121343612671\n",
      "Loss: 2.134840965270996\n",
      "Loss: 2.148859977722168\n",
      "Loss: 2.116386890411377\n",
      "Loss: 2.117372751235962\n",
      "Loss: 2.1366848945617676\n",
      "Loss: 2.1433558464050293\n",
      "Loss: 2.12117862701416\n",
      "Loss: 2.0884740352630615\n",
      "Loss: 2.143277883529663\n",
      "Loss: 2.1322946548461914\n",
      "Loss: 2.132072925567627\n",
      "Loss: 2.144286632537842\n",
      "Loss: 2.106708526611328\n",
      "Loss: 2.1155123710632324\n",
      "Loss: 2.1661994457244873\n",
      "Loss: 2.1089916229248047\n",
      "Loss: 2.1179490089416504\n",
      "Loss: 2.0814645290374756\n",
      "Loss: 2.1387994289398193\n",
      "Loss: 2.151228904724121\n",
      "Loss: 2.1125428676605225\n",
      "Loss: 2.0901284217834473\n",
      "Loss: 2.1537811756134033\n",
      "Loss: 2.08687686920166\n",
      "Loss: 2.107072353363037\n",
      "Loss: 2.146207094192505\n",
      "Loss: 2.1315040588378906\n",
      "Loss: 2.1451218128204346\n",
      "Loss: 2.1455907821655273\n",
      "Loss: 2.116384983062744\n",
      "Loss: 2.1311726570129395\n",
      "Loss: 2.099050521850586\n",
      "Loss: 2.1150553226470947\n",
      "Loss: 2.1276421546936035\n",
      "Loss: 2.1586804389953613\n",
      "Loss: 2.1264069080352783\n",
      "Loss: 2.1380605697631836\n",
      "Loss: 2.126694679260254\n",
      "Loss: 2.1125802993774414\n",
      "Loss: 2.1124863624572754\n",
      "Loss: 2.1137032508850098\n",
      "Loss: 2.1081326007843018\n",
      "Loss: 2.1331777572631836\n",
      "Loss: 2.123345136642456\n",
      "Loss: 2.1407179832458496\n",
      "Loss: 2.1192708015441895\n",
      "Loss: 2.1297779083251953\n",
      "Loss: 2.095825672149658\n",
      "Loss: 2.13090181350708\n",
      "Loss: 2.099500894546509\n",
      "Loss: 2.0816729068756104\n",
      "Loss: 2.117436408996582\n",
      "Loss: 2.1442196369171143\n",
      "Loss: 2.114320755004883\n",
      "Loss: 2.129157304763794\n",
      "Loss: 2.101830005645752\n",
      "Loss: 2.1547927856445312\n",
      "Loss: 2.129709005355835\n",
      "Loss: 2.1000328063964844\n",
      "Loss: 2.1134395599365234\n",
      "Loss: 2.11509370803833\n",
      "Loss: 2.113593578338623\n",
      "Loss: 2.102665424346924\n",
      "Loss: 2.11489200592041\n",
      "Loss: 2.1429600715637207\n",
      "Loss: 2.0965518951416016\n",
      "Loss: 2.1279845237731934\n",
      "Loss: 2.0961060523986816\n",
      "Loss: 2.132939338684082\n",
      "Loss: 2.1271438598632812\n",
      "Loss: 2.1351318359375\n",
      "Loss: 2.131092071533203\n",
      "Loss: 2.110297203063965\n",
      "Loss: 2.1513288021087646\n",
      "Loss: 2.0936758518218994\n",
      "Loss: 2.102611780166626\n",
      "Loss: 2.143610954284668\n",
      "Loss: 2.1480813026428223\n",
      "Loss: 2.115203619003296\n",
      "Loss: 2.1231703758239746\n",
      "Loss: 2.0976500511169434\n",
      "Loss: 2.127185583114624\n",
      "Loss: 2.1151084899902344\n",
      "Loss: 2.117140054702759\n",
      "Loss: 2.1149237155914307\n",
      "Loss: 2.1012113094329834\n",
      "Loss: 2.128328800201416\n",
      "Loss: 2.097719192504883\n",
      "Loss: 2.128333568572998\n",
      "Loss: 2.1117076873779297\n",
      "Loss: 2.1004230976104736\n",
      "Loss: 2.0991759300231934\n",
      "Loss: 2.1003336906433105\n",
      "Loss: 2.1190338134765625\n",
      "Loss: 2.181013584136963\n",
      "Loss: 2.10701322555542\n",
      "Loss: 2.2061712741851807\n",
      "Loss: 2.1470255851745605\n",
      "Loss: 2.1760523319244385\n",
      "Loss: 2.122542142868042\n",
      "Loss: 2.145085573196411\n",
      "Loss: 2.0983755588531494\n",
      "Loss: 2.1118836402893066\n",
      "Loss: 2.166069746017456\n",
      "Loss: 2.136213779449463\n",
      "Loss: 2.114325523376465\n",
      "Loss: 2.0964481830596924\n",
      "Loss: 2.106886625289917\n",
      "Loss: 2.144371509552002\n",
      "Loss: 2.104583740234375\n",
      "Loss: 2.128324508666992\n",
      "Loss: 2.1267566680908203\n",
      "Loss: 2.1080808639526367\n",
      "Loss: 2.1121773719787598\n",
      "Loss: 2.1143505573272705\n",
      "Loss: 2.0862863063812256\n",
      "Loss: 2.095918655395508\n",
      "Loss: 2.1402037143707275\n",
      "Loss: 2.1415324211120605\n",
      "Loss: 2.095193862915039\n",
      "Loss: 2.1102824211120605\n",
      "Loss: 2.110856294631958\n",
      "Loss: 2.0812525749206543\n",
      "Loss: 2.0959486961364746\n",
      "Loss: 2.0812225341796875\n",
      "Loss: 2.139329433441162\n",
      "Loss: 2.130854845046997\n",
      "Loss: 2.0958166122436523\n",
      "Loss: 2.0798606872558594\n",
      "Loss: 2.110692262649536\n",
      "Loss: 2.0791451930999756\n",
      "Loss: 2.126401662826538\n",
      "Loss: 2.0956599712371826\n",
      "Loss: 2.1416778564453125\n",
      "Loss: 2.0957822799682617\n",
      "Loss: 2.126535415649414\n",
      "Loss: 2.0950276851654053\n",
      "Loss: 2.094897985458374\n",
      "Loss: 2.1106724739074707\n",
      "Loss: 2.0964200496673584\n",
      "Loss: 2.1121654510498047\n",
      "Loss: 2.110067844390869\n",
      "Loss: 2.0957114696502686\n",
      "Loss: 2.126193046569824\n",
      "Loss: 2.0955810546875\n",
      "Loss: 2.1413912773132324\n",
      "Loss: 2.095349073410034\n",
      "Loss: 2.1428351402282715\n",
      "Loss: 2.094813823699951\n",
      "Loss: 2.1255807876586914\n",
      "Loss: 2.140657424926758\n",
      "Loss: 2.1265783309936523\n",
      "Loss: 2.1419553756713867\n",
      "Loss: 2.141204833984375\n",
      "Loss: 2.1415302753448486\n",
      "Loss: 2.1109671592712402\n",
      "Loss: 2.0959370136260986\n",
      "Loss: 2.1253957748413086\n",
      "Loss: 2.1418237686157227\n",
      "Loss: 2.140383720397949\n",
      "Loss: 2.0957672595977783\n",
      "Loss: 2.0944600105285645\n",
      "Loss: 2.1712024211883545\n",
      "Loss: 2.147878646850586\n",
      "Loss: 2.140763759613037\n",
      "Loss: 2.1421566009521484\n",
      "Loss: 2.110114812850952\n",
      "Loss: 2.1724750995635986\n",
      "Loss: 2.137531042098999\n",
      "Loss: 2.0948219299316406\n",
      "Loss: 2.0951225757598877\n",
      "Loss: 2.133021831512451\n",
      "Loss: 2.125789165496826\n",
      "Loss: 2.0794856548309326\n",
      "Loss: 2.1564817428588867\n",
      "Loss: 2.1711645126342773\n",
      "Loss: 2.126504421234131\n",
      "Loss: 2.094940662384033\n",
      "Loss: 2.0973589420318604\n",
      "Loss: 2.1101725101470947\n",
      "Loss: 2.0818581581115723\n",
      "Loss: 2.12605619430542\n",
      "Loss: 2.1135964393615723\n",
      "Loss: 2.126173496246338\n",
      "Loss: 2.0799741744995117\n",
      "Loss: 2.1252782344818115\n",
      "Loss: 2.095191717147827\n",
      "Loss: 2.078871965408325\n",
      "Loss: 2.1568477153778076\n",
      "Loss: 2.111145496368408\n",
      "Loss: 2.094425678253174\n",
      "Loss: 2.09502911567688\n",
      "Loss: 2.081261157989502\n",
      "Loss: 2.094872236251831\n",
      "Loss: 2.14933443069458\n",
      "Loss: 2.110802173614502\n",
      "Loss: 2.132174253463745\n",
      "Loss: 2.0798163414001465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.1425440311431885\n",
      "Loss: 2.12508487701416\n",
      "Loss: 2.125429153442383\n",
      "Loss: 2.1409106254577637\n",
      "Loss: 2.09523868560791\n",
      "Loss: 2.0798206329345703\n",
      "Loss: 2.080411911010742\n",
      "Loss: 2.110106945037842\n",
      "Loss: 2.127182960510254\n",
      "Loss: 2.1103262901306152\n",
      "Loss: 2.0950188636779785\n",
      "Loss: 2.155172824859619\n",
      "Loss: 2.096759080886841\n",
      "Loss: 2.080007553100586\n",
      "Loss: 2.126373291015625\n",
      "Loss: 2.12546968460083\n",
      "Loss: 2.110893487930298\n",
      "Loss: 2.1258935928344727\n",
      "Loss: 2.1259286403656006\n",
      "Loss: 2.1554458141326904\n",
      "Loss: 2.0791947841644287\n",
      "Loss: 2.12530517578125\n",
      "Loss: 2.110015869140625\n",
      "Loss: 2.126112461090088\n",
      "Loss: 2.0814876556396484\n",
      "Loss: 2.109929084777832\n",
      "Loss: 2.118425130844116\n",
      "Loss: 2.0956928730010986\n",
      "Loss: 2.169429063796997\n",
      "Loss: 2.1416478157043457\n",
      "Loss: 2.0952506065368652\n",
      "Loss: 2.09515380859375\n",
      "Loss: 2.1598525047302246\n",
      "Loss: 2.140629291534424\n",
      "Loss: 2.1259994506835938\n",
      "Loss: 2.079000473022461\n",
      "Loss: 2.0949554443359375\n",
      "Loss: 2.091393232345581\n",
      "Loss: 2.1105098724365234\n",
      "Loss: 2.1567864418029785\n",
      "Loss: 2.0973153114318848\n",
      "Loss: 2.1281261444091797\n",
      "Loss: 2.1255335807800293\n",
      "Loss: 2.11368989944458\n",
      "Loss: 2.1226282119750977\n",
      "Loss: 2.0942039489746094\n",
      "Loss: 2.1294338703155518\n",
      "Loss: 2.0951457023620605\n",
      "Loss: 2.125542640686035\n",
      "Loss: 2.080775737762451\n",
      "Loss: 2.172821044921875\n",
      "Loss: 2.1111459732055664\n",
      "Loss: 2.125290870666504\n",
      "Loss: 2.1256418228149414\n",
      "Loss: 2.1258065700531006\n",
      "Loss: 2.0951361656188965\n",
      "Loss: 2.079604387283325\n",
      "Loss: 2.145153522491455\n",
      "Loss: 2.141411542892456\n",
      "Loss: 2.0793297290802\n",
      "Loss: 2.1411421298980713\n",
      "Loss: 2.141511917114258\n",
      "Loss: 2.1115918159484863\n",
      "Loss: 2.125288963317871\n",
      "Loss: 2.110170364379883\n",
      "Loss: 2.098069667816162\n",
      "Loss: 2.118655204772949\n",
      "Loss: 2.1118874549865723\n",
      "Loss: 2.1406378746032715\n",
      "Loss: 2.102210283279419\n",
      "Loss: 2.145565986633301\n",
      "Loss: 2.1254258155822754\n",
      "Loss: 2.1099047660827637\n",
      "Loss: 2.110232353210449\n",
      "Loss: 2.1408939361572266\n",
      "Loss: 2.144770622253418\n",
      "Loss: 2.1100552082061768\n",
      "Loss: 2.110278844833374\n",
      "Loss: 2.110015392303467\n",
      "Loss: 2.0982372760772705\n",
      "Loss: 2.0940470695495605\n",
      "Loss: 2.129943370819092\n",
      "Loss: 2.1252427101135254\n",
      "Loss: 2.0946156978607178\n",
      "Loss: 2.132190465927124\n",
      "Loss: 2.0988337993621826\n",
      "Loss: 2.156097412109375\n",
      "Loss: 2.141160726547241\n",
      "Loss: 2.12697434425354\n",
      "Loss: 2.125643253326416\n",
      "Loss: 2.110966205596924\n",
      "Loss: 2.1573638916015625\n",
      "Loss: 2.1258811950683594\n",
      "Loss: 2.1113533973693848\n",
      "Loss: 2.140443801879883\n",
      "Loss: 2.0794224739074707\n",
      "Loss: 2.1326003074645996\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(len(np.array(list(zip(x_train, y_train)))))\n",
    "batches = batch_iter(list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "for batch in batches:\n",
    "    x_batch, y_batch = list(zip(*batch))\n",
    "    _, cost = sess.run([train_op,loss_op], feed_dict={input_x: x_batch,input_y: y_batch})\n",
    "    print( \"Loss: {}\".format(cost))\n",
    "    test_accuracy = sess.run(accuracy,feed_dict={input_x: x_batch,input_y: y_batch})\n",
    "    #print(\"test accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7532, 21273)\n",
      "(7532, 20)\n",
      "7532\n",
      "(7532, 2)\n",
      "test accuracy: 0.75\n",
      "test accuracy: 0.734375\n",
      "test accuracy: 0.71875\n",
      "test accuracy: 0.828125\n",
      "test accuracy: 0.765625\n",
      "test accuracy: 0.78125\n",
      "test accuracy: 0.8125\n",
      "test accuracy: 0.75\n",
      "test accuracy: 0.734375\n",
      "test accuracy: 0.75\n",
      "test accuracy: 0.75\n",
      "test accuracy: 0.734375\n",
      "test accuracy: 0.75\n",
      "test accuracy: 0.78125\n",
      "test accuracy: 0.78125\n",
      "test accuracy: 0.71875\n",
      "test accuracy: 0.828125\n",
      "test accuracy: 0.828125\n",
      "test accuracy: 0.828125\n",
      "test accuracy: 0.734375\n",
      "test accuracy: 0.765625\n",
      "test accuracy: 0.8125\n",
      "test accuracy: 0.8125\n",
      "test accuracy: 0.796875\n",
      "test accuracy: 0.765625\n",
      "test accuracy: 0.75\n",
      "test accuracy: 0.859375\n",
      "test accuracy: 0.8125\n",
      "test accuracy: 0.796875\n",
      "test accuracy: 0.828125\n",
      "test accuracy: 0.765625\n",
      "test accuracy: 0.765625\n",
      "test accuracy: 0.71875\n",
      "test accuracy: 0.71875\n",
      "test accuracy: 0.765625\n",
      "test accuracy: 0.765625\n",
      "test accuracy: 0.75\n",
      "test accuracy: 0.734375\n",
      "test accuracy: 0.71875\n",
      "test accuracy: 0.78125\n",
      "test accuracy: 0.78125\n",
      "test accuracy: 0.6875\n",
      "test accuracy: 0.765625\n",
      "test accuracy: 0.765625\n",
      "test accuracy: 0.78125\n",
      "test accuracy: 0.734375\n",
      "test accuracy: 0.8125\n",
      "test accuracy: 0.796875\n",
      "test accuracy: 0.796875\n",
      "test accuracy: 0.734375\n",
      "test accuracy: 0.78125\n",
      "test accuracy: 0.75\n",
      "test accuracy: 0.75\n",
      "test accuracy: 0.84375\n",
      "test accuracy: 0.6875\n",
      "test accuracy: 0.84375\n",
      "test accuracy: 0.734375\n",
      "test accuracy: 0.734375\n",
      "test accuracy: 0.828125\n",
      "test accuracy: 0.828125\n",
      "test accuracy: 0.6875\n",
      "test accuracy: 0.75\n",
      "test accuracy: 0.765625\n",
      "test accuracy: 0.765625\n",
      "test accuracy: 0.75\n",
      "test accuracy: 0.671875\n",
      "test accuracy: 0.703125\n",
      "test accuracy: 0.75\n",
      "test accuracy: 0.75\n",
      "test accuracy: 0.75\n",
      "test accuracy: 0.765625\n",
      "test accuracy: 0.75\n",
      "test accuracy: 0.796875\n",
      "test accuracy: 0.78125\n",
      "test accuracy: 0.78125\n",
      "test accuracy: 0.78125\n",
      "test accuracy: 0.796875\n",
      "test accuracy: 0.71875\n",
      "test accuracy: 0.78125\n",
      "test accuracy: 0.8125\n",
      "test accuracy: 0.78125\n",
      "test accuracy: 0.78125\n",
      "test accuracy: 0.765625\n",
      "test accuracy: 0.671875\n",
      "test accuracy: 0.84375\n",
      "test accuracy: 0.765625\n",
      "test accuracy: 0.640625\n",
      "test accuracy: 0.78125\n",
      "test accuracy: 0.75\n",
      "test accuracy: 0.859375\n",
      "test accuracy: 0.703125\n",
      "test accuracy: 0.671875\n",
      "test accuracy: 0.734375\n",
      "test accuracy: 0.8125\n",
      "test accuracy: 0.796875\n",
      "test accuracy: 0.703125\n",
      "test accuracy: 0.75\n",
      "test accuracy: 0.765625\n",
      "test accuracy: 0.71875\n",
      "test accuracy: 0.734375\n",
      "test accuracy: 0.796875\n",
      "test accuracy: 0.765625\n",
      "test accuracy: 0.796875\n",
      "test accuracy: 0.75\n",
      "test accuracy: 0.6875\n",
      "test accuracy: 0.765625\n",
      "test accuracy: 0.78125\n",
      "test accuracy: 0.765625\n",
      "test accuracy: 0.84375\n",
      "test accuracy: 0.78125\n",
      "test accuracy: 0.6875\n",
      "test accuracy: 0.8125\n",
      "test accuracy: 0.78125\n",
      "test accuracy: 0.734375\n",
      "test accuracy: 0.71875\n",
      "test accuracy: 0.8125\n",
      "test accuracy: 0.75\n",
      "test accuracy: 0.7045454978942871\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "sum1=0\n",
    "num1=0\n",
    "print(len(np.array(list(zip(x_test, y_test)))))\n",
    "batches = batch_iter(list(zip(x_test, y_test)), batch_size, 1)\n",
    "for batch in batches:\n",
    "    num1=num1+1\n",
    "    x_batch, y_batch = list(zip(*batch))\n",
    "    test_accuracy = sess.run(accuracy,feed_dict={input_x: x_batch,input_y: y_batch})\n",
    "    print(\"test accuracy: {}\".format(test_accuracy))\n",
    "    #acc=np.mean(idx==true_idx)\n",
    "    #print(acc)\n",
    "    sum1=sum1+test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "90.1576704979\n",
      "76.4048055067\n"
     ]
    }
   ],
   "source": [
    "print(num1)\n",
    "print(sum1)\n",
    "total1=sum1*100/num1\n",
    "print(total1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 21273)\n",
      "(11314, 20)\n",
      "11314\n",
      "(11314, 2)\n",
      "train accuracy: 1.0\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.921875\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 1.0\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.921875\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.90625\n",
      "train accuracy: 1.0\n",
      "train accuracy: 1.0\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.90625\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 1.0\n",
      "train accuracy: 0.921875\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 1.0\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 1.0\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.90625\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.921875\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 1.0\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.921875\n",
      "train accuracy: 1.0\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.921875\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.921875\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.90625\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.921875\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 1.0\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 1.0\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 1.0\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.921875\n",
      "train accuracy: 1.0\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.921875\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.921875\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.921875\n",
      "train accuracy: 0.96875\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.984375\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.9375\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 0.953125\n",
      "train accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "sum2=0\n",
    "num2=0\n",
    "print(len(np.array(list(zip(x_train, y_train)))))\n",
    "batches = batch_iter(list(zip(x_train, y_train)), batch_size, 1)\n",
    "for batch in batches:\n",
    "    num2=num2+1\n",
    "    x_batch, y_batch = list(zip(*batch))\n",
    "    train_accuracy = sess.run(accuracy,feed_dict={input_x: x_batch,input_y: y_batch})\n",
    "    print(\"train accuracy: {}\".format(train_accuracy))\n",
    "    #acc=np.mean(idx==true_idx)\n",
    "    #print(acc)\n",
    "    sum2=sum2+train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n",
      "170.625\n",
      "96.3983050847\n"
     ]
    }
   ],
   "source": [
    "print(num2)\n",
    "print(sum2)\n",
    "total2=sum2*100/num2\n",
    "print(total2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
